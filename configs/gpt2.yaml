

tokenizer:
  pretrained_model_name_or_path: gpt2
  use_slow_tokenizer: True

model:
  pretrained_model_name_or_path: gpt2
  distributed_mode: native_ddp
  trust_remote_code: False
  pretrained: False
  init_device: cpu
  native_amp: True
  mixed_precision: bf16
  enable_dynamo: False

distributed:
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1

dataset:
  dataset_name: wikitext
  dataset_config_name: wikitext-2-raw-v1
  block_size: 512

regularization:
  weight_decay: 0.01

learning_rate:
  lr: 5e-5
  lr_decay_style: linear
  lr_decay_iters: null
  lr_decay_samples: null
  lr_warmup_iters: 0
  lr_warmup_samples: 0

training:
  micro_batch_size: 16
  global_batch_size: null
  gradient_accumulation_steps: 1
  rampup_batch_size: null
  train_iters: 10000
  train_epochs: null
  train_samples: null
  lr_warmup_iters: null
  log_interval: 1
  tensorboard_dir: './tensorboard_log'
  tensorboard_queue_size: 10

checkpointing:
  save: ""
  save_interval: null
  load: ""

logging:
  timing_log_level: 0
  timing_log_option: minmax
  log_params_norm: null
  log_num_zeros_in_grad: null
  log_timers_to_tensorboard: True
  log_batch_size_to_tensorboard: True
  log_learning_rate_to_tensorboard: True
  log_loss_scale_to_tensorboard: True
  log_validation_ppl_to_tensorboard: True
  log_memory_to_tensorboard: True
  log_world_size_to_tensorboard: True
  no_barrier_with_level_1_timing: null
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
