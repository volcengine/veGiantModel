
tokenizer:
  pretrained_model_name_or_path: decapoda-research/llama-7b-hf
  use_slow_tokenizer: True

model:
  pretrained_model_name_or_path: decapoda-research/llama-7b-hf
  distributed_mode: fsdp
  trust_remote_code: False
  pretrained: True
  init_device: cpu
  enable_flash_attn: True

distributed:
  strategy: fsdp
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1

dataset:
  train_file: alpaca_data.json
  block_size: 1024

regularization:
  weight_decay: 0.01

learning_rate:
  lr: 5e-5
  lr_decay_style: linear
  lr_decay_iters: null
  lr_decay_samples: null
  lr_warmup_iters: 0
  lr_warmup_samples: 0

training:
  micro_batch_size: 2
  global_batch_size: null
  gradient_accumulation_steps: 4
  rampup_batch_size: null
  train_iters: 10000
  train_epochs: null
  train_samples: null
  lr_warmup_iters: null
  log_interval: 1
  tensorboard_dir: './tensorboard_log'
  tensorboard_queue_size: 10

checkpointing:
  save: ""
  save_interval: null
  load: ""

logging:
  timing_log_level: 0
  timing_log_option: minmax
  log_params_norm: null
  log_num_zeros_in_grad: null
  no_barrier_with_level_1_timing: null
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  log_timers_to_tensorboard: True
  log_batch_size_to_tensorboard: True
  log_learning_rate_to_tensorboard: True
  log_loss_scale_to_tensorboard: True
  log_validation_ppl_to_tensorboard: True
  log_memory_to_tensorboard: True
  log_world_size_to_tensorboard: True
